{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the GeoQuery dataset ingest pipelines documentation.</p> <p>This site includes guides for writing new dataset pipelines and deploying them to Prefect and Kubernetes, as well as documentation for our <code>data_manager</code> framework.</p> <p>Check Out GeoQuery!</p> <p>This is internal documentation, intended for use by GeoQuery developers at AidData. Looking for GeoQuery?</p> <p>Visit GeoQuery </p>"},{"location":"dataset_guide/","title":"Writing Pipelines","text":"<p>Info</p> <p>This guide is written with a general audience in mind, including folks who are new to data pipelines or programming in Python.</p> <p>GeoQuery is, to end users, a website that allows them to download prepared geospatial data. On the backend side, our job is to make sure this data is available (and current). We collect data from various sources, converting it to a standard format, and ingest it into the system that runs the website.</p>"},{"location":"dataset_guide/#overview","title":"Overview","text":"<p>It's important to make a plan before writing a data pipeline. Where is the data coming from, and where does it need to go? Here is an overview of what this looks like:</p> <ol> <li> <p>Read configuration</p> <ul> <li>Where to store downloaded and processed data?</li> <li>What years of data should be downloaded?</li> <li>If there is already a downloaded file, should it be overwritten?</li> </ul> </li> <li> <p>Download data</p> <ul> <li>If authentication is required in order to download, we do so</li> <li>Make sure every file gets downloaded correctly, retrying if necessary</li> <li>If the downloaded data is compressed (e.g. a .zip file), decompress it</li> </ul> </li> <li> <p>Process data</p> <ul> <li>Read original file format</li> <li>If necessary, apply any filters or quality assurance logic to the data</li> <li>Write it out in a standard format (COG, more on that later)</li> </ul> </li> </ol> <p>Loading this data into the website infrastucture is done separately so that we can manually check everything before publishing it.</p>"},{"location":"dataset_guide/adding_boilerplate/","title":"Adding Boilerplate","text":"<p>There is some boilerplate code we add to the bottom of all dataset scripts (after the Dataset definition) to set them up for deployment.</p>"},{"location":"dataset_guide/adding_boilerplate/#flow-definition","title":"Flow Definition","text":"<p>We add a Prefect flow definition that allows this dataset to be deployed to Prefect. Here is the code, with placeholder names for the <code>Dataset</code> and <code>BaseDatasetConfiguration</code>:</p> <pre><code>try:\n    from prefect import flow\nexcept:\n    pass\nelse:\n\n    @flow\n    def name_of_dataset(config: DatasetConfigurationName):\n        DatasetClassName(config).run(config.run)\n</code></pre>"},{"location":"dataset_guide/adding_boilerplate/#main-function","title":"Main Function","text":"<p>We use the common <code>if __name__ == \"__main__\"</code> syntax to instantiate the dataset and run it if the <code>main.py</code> script is run directly. Here is the template, to applied similarly to the one above:</p> <pre><code>if __name__ == \"__main__\":\n    config = get_config(DatasetConfigurationName)\n    DatasetClassName(config).run(config.run)\n</code></pre>"},{"location":"dataset_guide/dataset_class/","title":"Overview of the <code>Dataset</code> class","text":"<p>The idea behind the <code>Dataset</code> class is that it represents the complete logic of a dataset import, </p> <p>\"provide a means of bundling data and functionality together.\"</p>"},{"location":"dataset_guide/dataset_class/#main","title":"<code>main()</code>","text":"<p>When a dataset is run (see \"Running a dataset\" below), <code>main()</code> gets called.</p> <p><code>main()</code> defines the flow of the dataset run, describing the order of each set of tasks.</p> <p>Usually <code>main()</code> is relatively small, calling <code>self.run_tasks()</code> for the download function, handling the results, and then passing those into a process task run. When first reading a <code>Dataset</code> file, this is a good place to start in order to understand the steps involved with running a dataset.</p>"},{"location":"dataset_guide/dataset_class/#run_tasks","title":"<code>run_tasks()</code>","text":"<p>...</p>"},{"location":"dataset_guide/dataset_class/#tmp_to_dst_file","title":"<code>tmp_to_dst_file()</code>","text":"<p>...</p>"},{"location":"dataset_guide/dev_env/","title":"Setting Up Your Environment","text":"<p>Before developing a dataset pipeline, you'll need a development environment with the appropriate packages installed.</p> <p>Note</p> <p>You will need to use a command-line interface to work with conda and other Python-related tools. A great resource for getting comfortable with the command-line is the MIT Missing Semester course, which is available for free online. If you use Microsoft Windows, please consider installing Windows Subsystem for Linux.</p>"},{"location":"dataset_guide/dev_env/#environment-management-system","title":"Environment Management System","text":"<p>When developing on your local machine, you'll likely need a system for compartmentalizing environments you use for different development projects. While this setup is entirely up to you, we've found success using conda. Another option is mamba is a faster alternative to conda that is fully compatible.</p> <p>Whichever tool you choose, follow its installation instructions before proceeding.</p>"},{"location":"dataset_guide/dev_env/#clone-the-geo-datasets-repository","title":"Clone the geo-datasets Repository","text":"<ol> <li>Make sure git is installed.</li> <li><code>cd</code> to the directory you'd like to clone geo-datasets into.    This can be <code>~/Documents</code>, for example</li> <li>Run <code>git clone git@github.com:aiddata/geo-datasets.git</code>.</li> <li><code>cd</code> into <code>geo-datasets</code>.</li> </ol>"},{"location":"dataset_guide/dev_env/#install-dependencies","title":"Install Dependencies","text":"<p>Note</p> <p>This section assumes you are using conda (or mamba). If you are using some other environment management system, you'll have to adapt these instructions accordingly.</p> <ol> <li>Create an environment for geo-datasets.    We usually name the environment \"geodataXXX\", replacing the \"XXX\" with the version of Python we are currently using.    At the time of writing, that was 3.11:    <pre><code>conda create -n geodata311 python=3.11\n</code></pre></li> <li>Activate your new environment    <pre><code>conda activate geodata311\n</code></pre></li> <li>Change directory to the <code>kubernetes/containers/job-runner</code> subdirectory of the geo-datasets repository    <pre><code>cd geo-datasets/kubernetes/containers/job-runner\n</code></pre></li> <li>Install Python packages used by the latest job runner    <pre><code>pip install -r requirements.txt\n</code></pre></li> </ol>"},{"location":"dataset_guide/running/","title":"Running Your Dataset","text":"<p>...</p>"},{"location":"dataset_guide/script_pieces/","title":"Structure of an Ingest Script","text":"<p>Most data pipelines we write for GeoQuery have a similar structure. Without delving into too many implementation specifics (yet!), let's take a look at the common elements of these scripts.</p>"},{"location":"dataset_guide/script_pieces/#download","title":"Download","text":"<p>The download script is responsible for retrieving the data from its source. Sometimes this is through some sort of API, other times it is through an FTP server, or maybe a file hosting service like Box. In many cases the download step takes the longest, since it requires transferring large amounts of data across the internet. It's also important for us to respect the data providers by keeping our requests to reasonable volume. For these reasons, it can be a challenge to write an efficient and reliable download script.</p>"},{"location":"dataset_guide/script_pieces/#example","title":"Example","text":"<p>Below is an example of some Python code that downloads a website, http://example.com, to /path/to/dst. Click on the plus signs to read annotations describing what is going on.</p> <pre><code>src_url = \"http://example.com\" # (1)!\ndst_path = \"/path/to/dst\" # (2)!\n\nwith requests.get(src_url, stream=True) as src: # (3)!\n    src.raise_for_status() # (4)!\n    with open(dst_path, \"wb\") as dst: # (5)!\n            dst.write(src.content) # (6)!\n</code></pre> <ol> <li>This variable is a string representing the URL to download.</li> <li>This variable is a string representing the filepath to download to.</li> <li>This <code>with</code> syntax opens a context manager using the requests library.    Within the indented block below, the <code>src</code> variable is an object that represents the request.    Context managers are very common (and useful!) in Python!</li> <li>This is a requests-specific function that raises an exception if the HTTP status code indicates an error.</li> <li>Another context manager!    This time, we are opening a file for writing using the built-in <code>open</code> function.</li> <li>This is the meat of this entire script, instructing Python to write the content from our request into the opened file.</li> </ol> <p>Download scripts for different websites can vary dramatically, so it's difficult to show one example that illustrates them all. That said, a common requirement is to provide an API token when making a request. Below is some code that builds upon the previous example, adding an API token to the request's HTTP headers:</p> <pre><code>token = \"XXXXX\" # (1)!\nsrc_url = \"http://example.com\"\ndst_path = \"/path/to/dst\"\n\n\n# dictionary of HTTP headers\nheaders = {\n    \"Authorization\": f\"Bearer {token}\",\n}\n\nwith requests.get(src_url, headers=headers, stream=True) as src: # (2)!\n    # raise an exception (fail this task) if HTTP response indicates that an error occured\n    src.raise_for_status()\n    with open(dst_path, \"wb\") as dst:\n            dst.write(src.content)\n</code></pre> <ol> <li>This variable is a string representing some API token for this website.</li> <li>Adding headers to these keyword arguments includes the <code>headers</code> dictionary in the HTTP headers of this request.</li> </ol>"},{"location":"dataset_guide/script_pieces/#checksums","title":"Checksums","text":"<p>Especially when we are downloading thousands of images at once, it's possible for a few to get corrupted in the chaos of networking. In some cases, the data source provides a checksum of the files, so that we can confirm that our copy is correct. When it's possible, this is great functionality to include in the download step. If the data has already been downloaded, it's faster to check that it matches a checksum rather than download it all over again. If it doesn't match the checksum, we can queue to be redownloaded before moving on to the processing stage.</p>"},{"location":"dataset_guide/script_pieces/#process","title":"Process","text":"<p>The primary work a processing task accomplishes is reading the raw data, and writing it into COG files. To accomplish this, we primarily use the rasterio package.</p> <p>One key thing to understand about rasterio is that it manages file read and write settings as dictionaries of variables, passed as keyword arguments to the <code>rasterio.open()</code> function. When reading a source file, this dictionary can be accessed at the meta attribute of the opened file object, e.g. <code>src.meta</code>. When writing an output file, this dictionary can be defined as keyword arguments in the <code>rasterio.open()</code> command.</p> <pre><code>import rasterio\n\nwith rasterio.open(src_path, \"r\") as src:\n    # src.meta is the profile of the source file\n    with rasterio.open(dst_path, \"w\", **profile) as dst:\n        src.write(dst)\n</code></pre> <p>It's important to make sure that the source data is converted properly to the desired format. Datasets have varied levels of complexity, and</p>"},{"location":"dataset_guide/script_pieces/#connective-tissue","title":"Connective tissue","text":"<p><code>get_config()</code> is a boilerplate function that we use to read configuration variables from a config file, usually <code>config.ini</code>. It lives outside of the class definition in <code>dataset_dir/main.py</code>, and returns a dictionary of configuation variable names and their values.</p> <p>flow.py is a boilerplate module that we add to each dataset's directory that defines a Prefect flow for that module. If imports the class definition from main.py and runs it using parameters passed to the flow.</p>"},{"location":"dataset_guide/tips/","title":"Tips &amp; Tricks","text":"<p>Below are some nice things to know as you're getting started.</p>"},{"location":"dataset_guide/tips/#pathlib","title":"pathlib","text":"<p>pathlib is a module built in to Python 3.4+ that makes working with filepaths easier.</p> <p>Without something like pathlib, it can be tempting to create file path strings using string concatenation, like this:</p> <pre><code>dst_path = \"/path/to/dst/\"\n\nyear_path = \"/path/to/dst/2014/median.tif\"\n</code></pre> <p>While this works well enough, creating new variables from scratch for many file increases the risk of spelling errors, makes paths difficult to edit en masse, and requires repeating yourself over and over.</p> <p>Here is an example of pathlib accomplishing the same thing: <pre><code>from pathlib import Path\n\ndst_dir = Path(/path/to/dst)\n\n# you can use slashes to combine parts of a path together\n# now year_dir is a Path object pointing to /path/to/dst/2014/median.tif\nyear_path = dst_dir / \"2014\" / \"median.tif\"\n\n# you can use Path.as_posix() to get a string from the path\nassert isinstance(year_path.as_posix(), str)\n</code></pre></p> <p>Here, if someone were to edit <code>dst_dir</code> on the third line to point somewhere else, it would automatically update <code>year_path</code> and any other paths that reference it. Another nifty feature is that <code>Path</code> objects provide functions for quick analysis:</p> <pre><code>from pathlib import Path\n\nexample_path = Path(\"/path/to/dst/hello_world.txt\")\n\nexample_path.parent # (1)!\nexample_path.exists() # (2)!\nexample_path.name # (3)!\nexample_path.stem # (4)!\n</code></pre> <ol> <li>This attribute is also a <code>Path</code> object, pointing to the /path/to/dst directory.</li> <li>This returns a boolean representing whether or not this path actually exists on your filesystem.</li> <li>This returns the string <code>\"hello_world.txt\"</code></li> <li>This returns the string <code>\"hello_world\"</code></li> </ol> <p>To see a full list of <code>pathlib</code> features, checkout out its documentation here.</p>"},{"location":"deployment_guide/","title":"Deploying Pipelines","text":"<p>Once we've written a pipeline, we need to run it on W&amp;M computers, so that we can keep a copy of each output on our AidData filesystems, and then load that data into the GeoQuery website itself.</p> <p>This section will go over how we deploy our pipelines using Prefect, and run those pipelines on Kubernetes.</p>"},{"location":"deployment_guide/build-container/","title":"Building the job-runner Container","text":"<p>Warning</p> <p>Please make sure your workspace is clean (all changes stashed or committed) to prevent building containers with uncommitted code.</p>"},{"location":"deployment_guide/build-container/#build-the-container","title":"Build the Container","text":"<p>Info</p> <p>This image now builds with an unusually high UID, to match that of jwhall's on the HPC systems. To build this container, you will likely need to increase the range value of the available subuids on your system. The easiest way to do this is to append a \"0\" to the end of your user's line in <code>/etc/subuid</code></p> <ol> <li><code>cd</code> to the <code>kubernetes/containers</code> directory of the geo-datasets repository    <pre><code>cd geo-datasets/kubernetes/containers\n</code></pre></li> <li>Build the container out of the <code>job-runner</code> directory    <pre><code>podman build --tag geodata-container job-runner/\n</code></pre></li> </ol>"},{"location":"deployment_guide/build-container/#push-the-container","title":"Push the Container","text":"<p>Warning</p> <p>Only Jacob can push to <code>jacobwhall/geodata-container</code> on Docker Hub, which should be fixed.</p> <ol> <li>Login to Docker Hub with podman<ol> <li>Run <code>podman login</code></li> <li>Enter your username and password when prompted</li> </ol> </li> <li>Determine the short hash for this commit</li> <li>Push the container to Docker using the short hash    <pre><code>podman push geodata-container docker.io/jacobwhall/geodata-container:XXXXXX\n</code></pre></li> </ol>"},{"location":"deployment_guide/helm-chart/","title":"Deploying the Helm Chart","text":"<p>The helm chart for geo-datasets is relatively straightforward. Below is a table of helm chart values, and what they control.</p>"},{"location":"deployment_guide/helm-chart/#required-values","title":"Required Values","text":"<p>These values must be specified when deploying this helm chart.</p> Key Type Description <code>prefect.apiURL</code> string Prefect Server API URL <code>prefect.apiKey</code> string Prefect Server API key"},{"location":"deployment_guide/helm-chart/#optional-values","title":"Optional Values","text":"<p>Usually these values should not be overridden when deploying this helm chart. If a permanent change needs to be made, consider updating the default value directly in the geo-datasets repository.</p> Key Type Description <code>workerContainer</code> string Image to use for workers <code>prefect.workPool</code> string Name of Prefect work pool <code>prefect.replicas</code> int Number of workers to deploy <code>prod.nfs.address</code> string IP address of NFS server <code>prod.nfs.mountPath</code> string Path on NFS server to mount <code>dev.enabled</code> bool Delete resource"},{"location":"deployment_guide/k8s-debug/","title":"Debugging Jobs in Kubernetes","text":"<p>Ideally, our datasets run without hitch and a green checkbox appears in the Prefect UI upon completion. Unfortunately this isn't always the case, and it's sometimes useful to get a picture of what's going on in the pods themselves as they run in kubernetes. Below are some tips for doing so.</p> <p>Info</p> <p>Replace <code>geo-datasets-namespace</code> in the commands below with the namespace you've deployed the geo-datasets helm chart to.</p>"},{"location":"deployment_guide/k8s-debug/#print-pod-logs","title":"Print Pod Logs","text":"<p>List the pods currently running (or recently errored/completed): <pre><code>kubectl get pods --namespace geo-datasets-namespace\n</code></pre></p> <p>Print the logs from a specific pod: <pre><code>kubectl logs name-of-pod --namespace geo-datasets-namespace\n</code></pre></p>"},{"location":"deployment_guide/prefect/","title":"Deploying to Prefect","text":"<ol> <li>Login to Prefect</li> <li>Make sure you have your conda environment activated    <pre><code>conda activate geodata311\n</code></pre></li> <li><code>cd</code> to the root of the repository</li> <li>Run <code>deploy.py</code> from the <code>scripts</code> directory, passing the name of the dataset directory (without \"/datasets/\") as an argument    <pre><code>python scripts/deploy.py esa_landcover\n</code></pre></li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>data_manager<ul> <li>configuration</li> <li>dataset</li> </ul> </li> </ul>"},{"location":"reference/data_manager/","title":"data_manager","text":"<p>This package provides a framework for running ingest pipelines for GeoQuery, consisting of base classes meant to be inherited by ingest scripts.</p>"},{"location":"reference/data_manager/configuration/","title":"configuration","text":""},{"location":"reference/data_manager/configuration/#data_manager.configuration.BaseDatasetConfiguration","title":"<code>BaseDatasetConfiguration</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>This is the class that should be imported into <code>main.py</code> files within dataset directories, and built upon with Dataset-specific parameters. Common examples are <code>overwrite_download</code>, <code>overwrite_processing</code>, or <code>year_list</code>.</p> Source code in <code>data_manager/configuration.py</code> <pre><code>class BaseDatasetConfiguration(BaseModel):\n    \"\"\"\n    This is the class that should be imported into\n    `main.py` files within dataset directories, and\n    built upon with Dataset-specific parameters.\n    Common examples are `overwrite_download`,\n    `overwrite_processing`, or `year_list`.\n    \"\"\"\n    run: RunParameters\n</code></pre>"},{"location":"reference/data_manager/configuration/#data_manager.configuration.RunParameters","title":"<code>RunParameters</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>This is a pydantic BaseModel that represents the run parameters for a Dataset. This model is consumed by Dataset.run() as settings for how to run the Dataset.</p> Source code in <code>data_manager/configuration.py</code> <pre><code>class RunParameters(BaseModel):\n    \"\"\"\n    This is a pydantic BaseModel that represents the run\n    parameters for a Dataset. This model is consumed by\n    Dataset.run() as settings for how to run the Dataset.\n    \"\"\"\n    backend: Literal[\"local\", \"mpi\", \"prefect\"] = \"prefect\"\n    task_runner: Literal[\n        \"concurrent\",\n        \"dask\",\n        \"hpc\",\n        \"kubernetes\",\n        \"sequential\",\n    ] = \"concurrent\"\n    run_parallel: bool = True\n    max_workers: Optional[int] = 4\n    bypass_error_wrapper: bool = False\n    threads_per_worker: Optional[int] = 1\n    # cores_per_process: Optional[int] = None\n    chunksize: int = 1\n    log_dir: str\n    logger_level: int = logging.INFO\n    retries: int = 3\n    retry_delay: int = 5\n    conda_env: str = \"geodata38\"\n</code></pre>"},{"location":"reference/data_manager/configuration/#data_manager.configuration.get_config","title":"<code>get_config(model, config_path='config.toml')</code>","text":"<p>Load the configuration for a Dataset.</p> <p>This function reads a TOML configuration file (usually <code>config.toml</code>) out of the same directory as the <code>main.py</code> file, and returns a <code>BaseDatasetConfiguration</code> model filled in with the values from that configuration file.</p> Source code in <code>data_manager/configuration.py</code> <pre><code>def get_config(\n    model: BaseDatasetConfiguration, config_path: Union[Path, str] = \"config.toml\"\n):\n    \"\"\"\n    Load the configuration for a Dataset.\n\n    This function reads a TOML configuration\n    file (usually `config.toml`) out of the\n    same directory as the `main.py` file, and\n    returns a `BaseDatasetConfiguration` model\n    filled in with the values from that\n    configuration file.\n    \"\"\"\n    config_path = Path(config_path)\n    if config_path.exists():\n        with open(config_path, \"rb\") as src:\n            return model.model_validate(tomllib.load(src))\n    else:\n        return FileNotFoundError(\"No TOML config file found for dataset.\")\n</code></pre>"},{"location":"reference/data_manager/dataset/","title":"dataset","text":""},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>ABC</code></p> <p>This is the base class for Datasets, providing functions that manage task runs and logs</p> Source code in <code>data_manager/dataset.py</code> <pre><code>class Dataset(ABC):\n    \"\"\"\n    This is the base class for Datasets, providing functions that manage task runs and logs\n    \"\"\"\n\n    backend: str\n    name: str\n    retries: int\n    retry_delay: int\n\n    @abstractmethod\n    def main(self):\n        \"\"\"\n        Dataset child classes must implement a main function\n        This is the function that is called when Dataset.run() is invoked\n        \"\"\"\n        raise NotImplementedError(\"Dataset classes must implement a main function\")\n\n    def get_logger(self):\n        \"\"\"\n        This function will return a logger that implements the Python logging API:\n        https://docs.python.org/3/library/logging.html\n\n        If you are using Prefect, the logs will be managed by Prefect\n        \"\"\"\n        if self.backend == \"prefect\":\n            from prefect import get_run_logger\n\n            return get_run_logger()\n        else:\n            return logging.getLogger(\"dataset\")\n\n    @contextmanager\n    def tmp_to_dst_file(\n        self,\n        final_dst: str | os.PathLike,\n        make_dst_dir: bool = False,\n        tmp_dir: Optional[str | os.PathLike] = None,\n        validate_cog: bool = False,\n    ):\n        logger = self.get_logger()\n\n        final_dst = Path(final_dst)\n\n        # make sure that final_dst parent directory exists\n        if not final_dst.parent.exists():\n            if make_dst_dir:\n                os.makedirs(final_dst.parent, exist_ok=True)\n            else:\n                raise FileNotFoundError(\n                    f\"Parent directory of requested filepath {str(final_dst)} does not exist.\"\n                )\n\n        tmp_sub_dir = mkdtemp(dir=tmp_dir)\n        _, tmp_path = mkstemp(dir=tmp_sub_dir)\n        logger.debug(\n            f\"Created temporary file {tmp_path} with final destination {str(final_dst)}\"\n        )\n        yield tmp_path\n\n        # validate Cloud Optimized GeoTIFF\n        # doing this before move because disk r/w is almost certainly faster\n        if validate_cog:\n            is_valid, errors, warnings = cog_validate(tmp_path)\n            if is_valid:\n                logger.info(\n                    f\"Successfully validated output COG {tmp_path} (destined for {str(final_dst)}))\"\n                )\n            else:\n                logger.exception(\n                    f\"Failed to validate COG {tmp_path} (destined for {str(final_dst)})\"\n                )\n            for error in errors:\n                logger.error(f\"Error encountered when validating COG: {error}\")\n            for warning in warnings:\n                logger.warning(f\"Warning encountered when validating COG: {warning}\")\n\n        # move file from tmp_path to final_dst\n        try:\n            logger.debug(f\"Attempting to move {tmp_path} to {str(final_dst)}\")\n            shutil.move(tmp_path, final_dst)\n        except Exception:\n            logger.exception(\n                f\"Failed to transfer temporary file {tmp_path} to final destination {str(final_dst)}\"\n            )\n        else:\n            logger.debug(\n                f\"Successfully transferred {tmp_path} to final destination {str(final_dst)}\"\n            )\n\n    def error_wrapper(self, func: Callable, args: Dict[str, Any]):\n        \"\"\"\n        This is the wrapper that is used when running individual tasks\n        It will always return a TaskResult!\n        \"\"\"\n        logger = self.get_logger()\n\n        for try_no in range(self.retries + 1):\n            try:\n                return TaskResult(0, \"Success\", args, func(*args))\n            except Exception as e:\n                if self.bypass_error_wrapper:\n                    logger.info(\n                        \"Task failed with exception, and error wrapper bypass enabled. Raising...\"\n                    )\n                    raise\n                if try_no &lt; self.retries:\n                    logger.error(f\"Task failed with exception (retrying): {repr(e)}\")\n                    time.sleep(self.retry_delay)\n                    continue\n                else:\n                    logger.error(f\"Task failed with exception (giving up): {repr(e)}\")\n                    return TaskResult(1, repr(e), args, None)\n\n    def run_serial_tasks(self, name, func: Callable, input_list: Iterable[Any]):\n        \"\"\"\n        Run tasks in serial (locally), given a function and list of inputs\n        This will always return a list of TaskResults!\n        \"\"\"\n        logger = self.get_logger()\n        logger.debug(f\"run_serial_tasks - input_list: {input_list}\")\n        return [self.error_wrapper(func, i) for i in input_list]\n\n    def run_concurrent_tasks(\n        self,\n        name: str,\n        func: Callable,\n        input_list: Iterable[Any],\n        force_sequential: bool,\n        max_workers: int = None,\n    ):\n        \"\"\"\n        Run tasks concurrently (locally), given a function a list of inputs\n        This will always return a list of TaskResults!\n        \"\"\"\n        if max_workers is None:\n            max_workers = self.max_workers\n        pool_size = 1 if force_sequential else max_workers\n        with multiprocessing.Pool(pool_size) as pool:\n            results = pool.starmap(\n                self.error_wrapper,\n                [(func, i) for i in input_list],\n                chunksize=self.chunksize,\n            )\n        return results\n\n    def run_prefect_tasks(\n        self,\n        name: str,\n        func: Callable,\n        input_list: Iterable[Any],\n        force_sequential: bool,\n    ):\n        \"\"\"\n        Run tasks using Prefect, using whichever task runner decided in self.run()\n        This will always return a list of TaskResults!\n        \"\"\"\n\n        from prefect import task\n\n        logger = self.get_logger()\n\n        task_wrapper = task(\n            func,\n            name=name,\n            retries=self.retries,\n            retry_delay_seconds=self.retry_delay,\n            persist_result=True,\n        )\n\n        futures = []\n        for i in input_list:\n            w = [i[1] for i in futures] if force_sequential else None\n            futures.append((i, task_wrapper.submit(*i, wait_for=w, return_state=False)))\n\n        results = []\n\n        states = [(i[0], i[1].wait()) for i in futures]\n\n        while states:\n            for ix, (inputs, state) in enumerate(states):\n                if state.is_completed():\n                    # print('complete', ix, inputs)\n                    logger.info(f\"complete - {ix} - {inputs}\")\n\n                    results.append(TaskResult(0, \"Success\", inputs, state.result()))\n                elif state.is_failed() or state.is_crashed() or state.is_cancelled():\n                    # print('fail', ix, inputs)\n                    logger.info(f\"fail - {ix} - {inputs}\")\n\n                    try:\n                        msg = repr(state.result(raise_on_failure=True))\n                    except Exception as e:\n                        msg = f\"Unable to retrieve error message - {e}\"\n                    results.append(TaskResult(1, msg, inputs, None))\n                else:\n                    # print('not ready', ix, inputs)\n                    continue\n                _ = states.pop(ix)\n            time.sleep(5)\n\n        # for inputs, future in futures:\n        #     state = future.wait(60*60*2)\n        #     if state.is_completed():\n        #         results.append(TaskResult(0, \"Success\", inputs, state.result()))\n        #     elif state.is_failed() or state.is_crashed():\n        #         try:\n        #             msg = repr(state.result(raise_on_failure=False))\n        #         except:\n        #             msg = \"Unable to retrieve error message\"\n        #         results.append(TaskResult(1, msg, inputs, None))\n        #     else:\n        #         pass\n\n        # while futures:\n        #     for ix, (inputs, future) in enumerate(futures):\n        #         state = future.get_state()\n        #         # print(repr(state))\n        #         # print(repr(future))\n        #         if state.is_completed():\n        #             print('complete', ix, inputs)\n        #             results.append(TaskResult(0, \"Success\", inputs, future.result()))\n        #         elif state.is_failed() or state.is_crashed() or state.is_cancelled():\n        #             print('fail', ix, inputs)\n        #             try:\n        #                 msg = repr(future.result(raise_on_failure=True))\n        #             except Exception as e:\n        #                 msg = f\"Unable to retrieve error message - {e}\"\n        #             results.append(TaskResult(1, msg, inputs, None))\n        #         else:\n        #             # print('not ready', ix, inputs)\n        #             continue\n        #         _ = futures.pop(ix)\n        #         # future.release()\n        #     time.sleep(5)\n\n        return results\n\n    def run_mpi_tasks(\n        self,\n        name: str,\n        func: Callable,\n        input_list: Iterable[Any],\n        force_sequential: bool,\n        max_workers: int = None,\n    ):\n        \"\"\"\n        Run tasks using MPI, requiring the use of `mpirun`\n        self.pool is an MPIPoolExecutor initialized by self.run()\n        This will always return a list of TaskResults!\n        \"\"\"\n        from mpi4py.futures import MPIPoolExecutor\n\n        if not max_workers:\n            max_workers = self.mpi_max_workers\n\n        with MPIPoolExecutor(\n            max_workers=max_workers, chunksize=self.chunksize\n        ) as pool:\n            futures = []\n            for i in input_list:\n                f = pool.submit(self.error_wrapper, func, i)\n                if force_sequential:\n                    wait([f])\n                futures.append(f)\n        return [f.result() for f in futures]\n\n    def run_tasks(\n        self,\n        func: Callable,\n        input_list: Iterable[Any],\n        allow_futures: bool = True,\n        name: Optional[str] = None,\n        retries: int = 3,\n        retry_delay: int = 60,\n        force_sequential: bool = False,\n        force_serial: bool = False,\n        max_workers: Optional[int] = None,\n        prefect_concurrency_tag: bool = None,\n        prefect_concurrency_task_value: bool = None,\n    ):\n        \"\"\"\n        Run a bunch of tasks, calling one of the above run_tasks functions\n        This is the function that should be called most often from self.main()\n        It will return a ResultTuple of TaskResults\n        \"\"\"\n\n        timestamp = datetime.today()\n\n        if not callable(func):\n            raise TypeError(\"Function passed to run_tasks is not callable\")\n\n        # Save global retry settings, and override with current values\n        old_retries, old_retry_delay = self.retries, self.retry_delay\n        self.retries, self.retry_delay = self.init_retries(retries, retry_delay)\n\n        logger = self.get_logger()\n\n        if name is None:\n            try:\n                name = func.__name__\n            except AttributeError:\n                logger.warning(\n                    \"No name given for task run, and function does not have a name (multiple unnamed functions may result in log files being overwritten)\"\n                )\n                name = \"unnamed\"\n        elif not isinstance(name, str):\n            raise TypeError(\"Name of task run must be a string\")\n\n        if max_workers is None:\n            max_workers = self.max_workers\n\n        if self.backend == \"serial\" or force_serial:\n            results = self.run_serial_tasks(name, func, input_list)\n        elif self.backend == \"concurrent\":\n            results = self.run_concurrent_tasks(\n                name, func, input_list, force_sequential, max_workers=max_workers\n            )\n        elif self.backend == \"prefect\":\n            from prefect.concurrency.asyncio import concurrency\n            with concurrency(prefect_concurrency_tag, occupy=prefect_concurrency_task_value):\n                results = self.run_prefect_tasks(name, func, input_list, force_sequential)\n\n        elif self.backend == \"mpi\":\n            results = self.run_mpi_tasks(name, func, input_list, force_sequential, max_workers=max_workers)\n        else:\n            raise ValueError(\n                \"Requested backend not recognized. Have you called this Dataset's run function?\"\n            )\n\n        if len(results) == 0:\n            raise ValueError(\n                f\"Task run {name} yielded no results. Did it receive any inputs?\"\n            )\n\n        success_count = sum(1 for r in results if r.status_code == 0)\n        error_count = len(results) - success_count\n        if error_count == 0:\n            logger.info(\n                f\"Task run {name} completed with {success_count} successes and no errors\"\n            )\n        else:\n            logger.warning(\n                f\"Task run {name} completed with {error_count} errors and {success_count} successes\"\n            )\n\n        # Restore global retry settings\n        self.retries, self.retry_delay = old_retries, old_retry_delay\n\n        return ResultTuple(results, name, timestamp)\n\n    def log_run(\n        self,\n        results,\n        expand_args: list = [],\n        expand_results: list = [],\n        time_format_str: str = \"%Y_%m_%d_%H_%M\",\n    ):\n        \"\"\"\n        Log a task run\n        Given a ResultTuple (usually from run_tasks), and save its logs to a CSV file\n        time_format_str sets the timestamp format to use in the CSV filename\n\n        expand_results is an optional set of labels for each item in TaskResult.result\n          - None values in expand_results will exclude that column from output\n          - if expand_results is an empty list, each TaskResult's result value will be\n            written as-is to a \"results\" column in the CSV\n        \"\"\"\n        time_str = results.timestamp.strftime(time_format_str)\n        log_file = self.log_dir / f\"{results.name}_{time_str}.csv\"\n\n        fieldnames = [\"status_code\", \"status_message\"]\n\n        should_expand_args = False\n        args_expansion_spec = []\n\n        for ai, ax in enumerate(expand_args):\n            if ax is not None:\n                should_expand_args = True\n                fieldnames.append(ax)\n                args_expansion_spec.append((ax, ai))\n\n        if not should_expand_args:\n            fieldnames.append(\"args\")\n\n        should_expand_results = False\n        results_expansion_spec = []\n\n        for ri, rx in enumerate(expand_results):\n            if rx is not None:\n                should_expand_results = True\n                fieldnames.append(rx)\n                results_expansion_spec.append((rx, ri))\n\n        if not should_expand_results:\n            fieldnames.append(\"results\")\n\n        rows_to_write = []\n\n        for r in results:\n            row = [r[0], r[1]]\n            if should_expand_args:\n                row.extend(\n                    [\n                        r[2][i] if r[2] is not None else None\n                        for _, i in args_expansion_spec\n                    ]\n                )\n            else:\n                row.append(r[2])\n\n            if should_expand_results:\n                row.extend(\n                    [\n                        r[3][i] if r[3] is not None else None\n                        for _, i in results_expansion_spec\n                    ]\n                )\n            else:\n                row.append(r[3])\n\n            rows_to_write.append(row)\n\n        with open(log_file, \"w\", newline=\"\") as lf:\n            writer = csv.writer(lf)\n            writer.writerow(fieldnames)\n            writer.writerows(rows_to_write)\n\n    def init_retries(self, retries: int, retry_delay: int, save_settings: bool = False):\n        \"\"\"\n        Given a number of task retries and a retry_delay,\n        checks to make sure those values are valid\n        (ints greater than or equal to zero), and\n        optionally sets class variables to keep their\n        settings\n        \"\"\"\n        if isinstance(retries, int):\n            if retries &lt; 0:\n                raise ValueError(\n                    \"Number of task retries must be greater than or equal to zero\"\n                )\n            elif save_settings:\n                self.retries = retries\n        else:\n            raise TypeError(\"retries must be an int greater than or equal to zero\")\n\n        if isinstance(retry_delay, int):\n            if retry_delay &lt; 0:\n                raise ValueError(\"Retry delay must be greater than or equal to zero\")\n            elif save_settings:\n                self.retry_delay = retry_delay\n        else:\n            raise TypeError(\n                \"retry_delay must be an int greater than or equal to zero, representing the number of seconds to wait before retrying a task\"\n            )\n\n        return retries, retry_delay\n\n    def _check_env_and_run(self):\n        \"\"\"\n        Check if $TMPDIR is in /local, log warning if it is\n        Then, run self.main()\n        \"\"\"\n        logger = self.get_logger()\n\n        try:\n            # $TMPDIR is the default temporary directory that deployments use to store and execute code\n            # is $TMPDIR set, and can we resolve it (find it on the filesystem)?\n            tmp_dir = Path(os.environ[\"TMPDIR\"]).resolve(strict=True)\n        except KeyError:\n            # KeyError if there is no such environment variable\n            logger.warning(\"No $TMPDIR environment variable found!\")\n        except FileNotFoundError:\n            # when we tried to resolve the path, the folder wasn't found on filesystem\n            logger.warning(\"$TMPDIR path not found!\")\n        else:\n            # /local points to local storage on W&amp;M HPC\n            slash_local = Path(\"/local\").resolve()\n            # is /local a parent dir of tmp_dir?\n            for p in tmp_dir.parents:\n                if p.resolve() == slash_local:\n                    logger.warning(\n                        \"$TMPDIR in /local, deployments won't be accessible to compute nodes.\"\n                    )\n\n        # run the dataset (self.main() should be defined in child class instance)\n        self.main()\n\n    def run(\n        self,\n        params: RunParameters,\n        **kwargs,\n    ):\n        \"\"\"\n        Run a dataset\n        Initializes class variables and chosen backend\n        This is how Datasets should usually be run\n        Eventually calls _check_env_and_run(), starting dataset (see below)\n        \"\"\"\n        logger = self.get_logger()\n\n        # get current timestamp and initialize log directory\n        timestamp = datetime.today()\n        time_format_str: str = \"%Y_%m_%d_%H_%M\"\n        time_str = timestamp.strftime(time_format_str)\n        self.log_dir = Path(params.log_dir) / time_str\n        os.makedirs(self.log_dir, exist_ok=True)\n\n        self.init_retries(params.retries, params.retry_delay, save_settings=True)\n\n        self.chunksize = params.chunksize\n\n        self.bypass_error_wrapper = params.bypass_error_wrapper\n\n        # Allow datasets to set their own default max_workers\n        if params.max_workers is None and hasattr(self, \"max_workers\"):\n            max_workers = self.max_workers\n        else:\n            max_workers = params.max_workers\n\n        # If dataset doesn't come with a name use its class name\n        if not self.name:\n            self.name = self._type()\n\n        if params.backend == \"prefect\":\n            self.backend = \"prefect\"\n\n            logger.info(f\"Running with {params.task_runner} task runner with {max_workers} workers\")\n\n            from prefect import flow\n            from prefect.task_runners import SequentialTaskRunner, ConcurrentTaskRunner#, ThreadPoolTaskRunner\n\n\n            if params.task_runner == \"sequential\":\n                tr = SequentialTaskRunner\n            elif params.task_runner == \"concurrent\" or params.task_runner is None:\n                # placeholder for actual ThreadPoolTaskRunner release in future Prefect versions\n                # tr = ThreadPoolTaskRunner(max_workers=max_workers)\n                tr = ConcurrentTaskRunner()\n\n            elif params.task_runner == \"dask\":\n                from prefect_dask import DaskTaskRunner\n\n                # if \"cluster\" in kwargs:\n                # del kwargs[\"cluster\"]\n                # if \"cluster_kwargs\" in kwargs:\n                # del kwargs[\"cluster_kwargs\"]\n\n                dask_cluster_kwargs = {\n                    \"n_workers\": max_workers,\n                    \"threads_per_worker\": params.threads_per_worker,\n                }\n                tr = DaskTaskRunner(cluster_kwargs=dask_cluster_kwargs)\n            elif params.task_runner == \"hpc\":\n                from hpc import HPCDaskTaskRunner\n\n                job_name = \"\".join(self.name.split())\n                tr = HPCDaskTaskRunner(\n                    num_procs=max_workers,\n                    job_name=job_name,\n                    log_dir=self.log_dir,\n                    **kwargs,\n                )\n            elif params.task_runner == \"kubernetes\":\n                from dask_kubernetes.operator import KubeCluster, make_cluster_spec\n                from prefect_dask import DaskTaskRunner\n\n                spec = make_cluster_spec(name=\"selector-example\", n_workers=2)\n                spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\n                    \"image\"\n                ] = \"docker.io/jacobwhall/geodata-dask\"\n                spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\n                    \"imagePullPolicy\"\n                ] = \"Always\"\n                spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\"env\"] = [\n                    {\n                        \"name\": \"DATA_MANAGER_VERSION\",\n                        \"value\": os.environ[\"DATA_MANAGER_VERSION\"],\n                    }\n                ]\n                spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\"volumeMounts\"] = [\n                    {\"name\": \"sciclone\", \"mountPath\": \"/sciclone\"}\n                ]\n\n                spec[\"spec\"][\"worker\"][\"spec\"][\"volumes\"] = [\n                    {\n                        \"name\": \"sciclone\",\n                        \"persistentVolumeClaim\": {\"claimName\": \"nova-geodata-prod\"},\n                    }\n                ]\n\n                dask_task_runner_kwargs = {\n                    \"cluster_class\": KubeCluster,\n                    \"cluster_kwargs\": {\n                        \"custom_cluster_spec\": spec,\n                    },\n                    \"adapt_kwargs\": {\n                        \"minimum\": 1,\n                        \"maximum\": max_workers,\n                    },\n                }\n                tr = DaskTaskRunner(**dask_task_runner_kwargs)\n            else:\n                raise ValueError(\"Prefect task runner not recognized\")\n\n            @flow(task_runner=tr, name=self.name)\n            def prefect_main_wrapper():\n                self._check_env_and_run()\n\n            prefect_main_wrapper()\n\n        else:\n            logger = logging.getLogger(\"dataset\")\n            logger.setLevel(params.logger_level)\n            logger.addHandler(logging.StreamHandler())\n\n            if params.backend == \"mpi\":\n                from mpi4py import MPI\n\n                comm = MPI.COMM_WORLD\n                rank = comm.Get_rank()\n                if rank != 0:\n                    return\n\n                self.backend = \"mpi\"\n                self.mpi_max_workers = max_workers\n\n                self._check_env_and_run()\n\n            elif params.backend == \"local\":\n                if params.run_parallel:\n                    self.backend = \"concurrent\"\n                else:\n                    self.backend = \"serial\"\n                self._check_env_and_run()\n\n            else:\n                raise ValueError(f\"Backend {params.backend} not recognized.\")\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.error_wrapper","title":"<code>error_wrapper(func, args)</code>","text":"<p>This is the wrapper that is used when running individual tasks It will always return a TaskResult!</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def error_wrapper(self, func: Callable, args: Dict[str, Any]):\n    \"\"\"\n    This is the wrapper that is used when running individual tasks\n    It will always return a TaskResult!\n    \"\"\"\n    logger = self.get_logger()\n\n    for try_no in range(self.retries + 1):\n        try:\n            return TaskResult(0, \"Success\", args, func(*args))\n        except Exception as e:\n            if self.bypass_error_wrapper:\n                logger.info(\n                    \"Task failed with exception, and error wrapper bypass enabled. Raising...\"\n                )\n                raise\n            if try_no &lt; self.retries:\n                logger.error(f\"Task failed with exception (retrying): {repr(e)}\")\n                time.sleep(self.retry_delay)\n                continue\n            else:\n                logger.error(f\"Task failed with exception (giving up): {repr(e)}\")\n                return TaskResult(1, repr(e), args, None)\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.get_logger","title":"<code>get_logger()</code>","text":"<p>This function will return a logger that implements the Python logging API: https://docs.python.org/3/library/logging.html</p> <p>If you are using Prefect, the logs will be managed by Prefect</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def get_logger(self):\n    \"\"\"\n    This function will return a logger that implements the Python logging API:\n    https://docs.python.org/3/library/logging.html\n\n    If you are using Prefect, the logs will be managed by Prefect\n    \"\"\"\n    if self.backend == \"prefect\":\n        from prefect import get_run_logger\n\n        return get_run_logger()\n    else:\n        return logging.getLogger(\"dataset\")\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.init_retries","title":"<code>init_retries(retries, retry_delay, save_settings=False)</code>","text":"<p>Given a number of task retries and a retry_delay, checks to make sure those values are valid (ints greater than or equal to zero), and optionally sets class variables to keep their settings</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def init_retries(self, retries: int, retry_delay: int, save_settings: bool = False):\n    \"\"\"\n    Given a number of task retries and a retry_delay,\n    checks to make sure those values are valid\n    (ints greater than or equal to zero), and\n    optionally sets class variables to keep their\n    settings\n    \"\"\"\n    if isinstance(retries, int):\n        if retries &lt; 0:\n            raise ValueError(\n                \"Number of task retries must be greater than or equal to zero\"\n            )\n        elif save_settings:\n            self.retries = retries\n    else:\n        raise TypeError(\"retries must be an int greater than or equal to zero\")\n\n    if isinstance(retry_delay, int):\n        if retry_delay &lt; 0:\n            raise ValueError(\"Retry delay must be greater than or equal to zero\")\n        elif save_settings:\n            self.retry_delay = retry_delay\n    else:\n        raise TypeError(\n            \"retry_delay must be an int greater than or equal to zero, representing the number of seconds to wait before retrying a task\"\n        )\n\n    return retries, retry_delay\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.log_run","title":"<code>log_run(results, expand_args=[], expand_results=[], time_format_str='%Y_%m_%d_%H_%M')</code>","text":"<p>Log a task run Given a ResultTuple (usually from run_tasks), and save its logs to a CSV file time_format_str sets the timestamp format to use in the CSV filename</p> <p>expand_results is an optional set of labels for each item in TaskResult.result   - None values in expand_results will exclude that column from output   - if expand_results is an empty list, each TaskResult's result value will be     written as-is to a \"results\" column in the CSV</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def log_run(\n    self,\n    results,\n    expand_args: list = [],\n    expand_results: list = [],\n    time_format_str: str = \"%Y_%m_%d_%H_%M\",\n):\n    \"\"\"\n    Log a task run\n    Given a ResultTuple (usually from run_tasks), and save its logs to a CSV file\n    time_format_str sets the timestamp format to use in the CSV filename\n\n    expand_results is an optional set of labels for each item in TaskResult.result\n      - None values in expand_results will exclude that column from output\n      - if expand_results is an empty list, each TaskResult's result value will be\n        written as-is to a \"results\" column in the CSV\n    \"\"\"\n    time_str = results.timestamp.strftime(time_format_str)\n    log_file = self.log_dir / f\"{results.name}_{time_str}.csv\"\n\n    fieldnames = [\"status_code\", \"status_message\"]\n\n    should_expand_args = False\n    args_expansion_spec = []\n\n    for ai, ax in enumerate(expand_args):\n        if ax is not None:\n            should_expand_args = True\n            fieldnames.append(ax)\n            args_expansion_spec.append((ax, ai))\n\n    if not should_expand_args:\n        fieldnames.append(\"args\")\n\n    should_expand_results = False\n    results_expansion_spec = []\n\n    for ri, rx in enumerate(expand_results):\n        if rx is not None:\n            should_expand_results = True\n            fieldnames.append(rx)\n            results_expansion_spec.append((rx, ri))\n\n    if not should_expand_results:\n        fieldnames.append(\"results\")\n\n    rows_to_write = []\n\n    for r in results:\n        row = [r[0], r[1]]\n        if should_expand_args:\n            row.extend(\n                [\n                    r[2][i] if r[2] is not None else None\n                    for _, i in args_expansion_spec\n                ]\n            )\n        else:\n            row.append(r[2])\n\n        if should_expand_results:\n            row.extend(\n                [\n                    r[3][i] if r[3] is not None else None\n                    for _, i in results_expansion_spec\n                ]\n            )\n        else:\n            row.append(r[3])\n\n        rows_to_write.append(row)\n\n    with open(log_file, \"w\", newline=\"\") as lf:\n        writer = csv.writer(lf)\n        writer.writerow(fieldnames)\n        writer.writerows(rows_to_write)\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.main","title":"<code>main()</code>  <code>abstractmethod</code>","text":"<p>Dataset child classes must implement a main function This is the function that is called when Dataset.run() is invoked</p> Source code in <code>data_manager/dataset.py</code> <pre><code>@abstractmethod\ndef main(self):\n    \"\"\"\n    Dataset child classes must implement a main function\n    This is the function that is called when Dataset.run() is invoked\n    \"\"\"\n    raise NotImplementedError(\"Dataset classes must implement a main function\")\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.run","title":"<code>run(params, **kwargs)</code>","text":"<p>Run a dataset Initializes class variables and chosen backend This is how Datasets should usually be run Eventually calls _check_env_and_run(), starting dataset (see below)</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def run(\n    self,\n    params: RunParameters,\n    **kwargs,\n):\n    \"\"\"\n    Run a dataset\n    Initializes class variables and chosen backend\n    This is how Datasets should usually be run\n    Eventually calls _check_env_and_run(), starting dataset (see below)\n    \"\"\"\n    logger = self.get_logger()\n\n    # get current timestamp and initialize log directory\n    timestamp = datetime.today()\n    time_format_str: str = \"%Y_%m_%d_%H_%M\"\n    time_str = timestamp.strftime(time_format_str)\n    self.log_dir = Path(params.log_dir) / time_str\n    os.makedirs(self.log_dir, exist_ok=True)\n\n    self.init_retries(params.retries, params.retry_delay, save_settings=True)\n\n    self.chunksize = params.chunksize\n\n    self.bypass_error_wrapper = params.bypass_error_wrapper\n\n    # Allow datasets to set their own default max_workers\n    if params.max_workers is None and hasattr(self, \"max_workers\"):\n        max_workers = self.max_workers\n    else:\n        max_workers = params.max_workers\n\n    # If dataset doesn't come with a name use its class name\n    if not self.name:\n        self.name = self._type()\n\n    if params.backend == \"prefect\":\n        self.backend = \"prefect\"\n\n        logger.info(f\"Running with {params.task_runner} task runner with {max_workers} workers\")\n\n        from prefect import flow\n        from prefect.task_runners import SequentialTaskRunner, ConcurrentTaskRunner#, ThreadPoolTaskRunner\n\n\n        if params.task_runner == \"sequential\":\n            tr = SequentialTaskRunner\n        elif params.task_runner == \"concurrent\" or params.task_runner is None:\n            # placeholder for actual ThreadPoolTaskRunner release in future Prefect versions\n            # tr = ThreadPoolTaskRunner(max_workers=max_workers)\n            tr = ConcurrentTaskRunner()\n\n        elif params.task_runner == \"dask\":\n            from prefect_dask import DaskTaskRunner\n\n            # if \"cluster\" in kwargs:\n            # del kwargs[\"cluster\"]\n            # if \"cluster_kwargs\" in kwargs:\n            # del kwargs[\"cluster_kwargs\"]\n\n            dask_cluster_kwargs = {\n                \"n_workers\": max_workers,\n                \"threads_per_worker\": params.threads_per_worker,\n            }\n            tr = DaskTaskRunner(cluster_kwargs=dask_cluster_kwargs)\n        elif params.task_runner == \"hpc\":\n            from hpc import HPCDaskTaskRunner\n\n            job_name = \"\".join(self.name.split())\n            tr = HPCDaskTaskRunner(\n                num_procs=max_workers,\n                job_name=job_name,\n                log_dir=self.log_dir,\n                **kwargs,\n            )\n        elif params.task_runner == \"kubernetes\":\n            from dask_kubernetes.operator import KubeCluster, make_cluster_spec\n            from prefect_dask import DaskTaskRunner\n\n            spec = make_cluster_spec(name=\"selector-example\", n_workers=2)\n            spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\n                \"image\"\n            ] = \"docker.io/jacobwhall/geodata-dask\"\n            spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\n                \"imagePullPolicy\"\n            ] = \"Always\"\n            spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\"env\"] = [\n                {\n                    \"name\": \"DATA_MANAGER_VERSION\",\n                    \"value\": os.environ[\"DATA_MANAGER_VERSION\"],\n                }\n            ]\n            spec[\"spec\"][\"worker\"][\"spec\"][\"containers\"][0][\"volumeMounts\"] = [\n                {\"name\": \"sciclone\", \"mountPath\": \"/sciclone\"}\n            ]\n\n            spec[\"spec\"][\"worker\"][\"spec\"][\"volumes\"] = [\n                {\n                    \"name\": \"sciclone\",\n                    \"persistentVolumeClaim\": {\"claimName\": \"nova-geodata-prod\"},\n                }\n            ]\n\n            dask_task_runner_kwargs = {\n                \"cluster_class\": KubeCluster,\n                \"cluster_kwargs\": {\n                    \"custom_cluster_spec\": spec,\n                },\n                \"adapt_kwargs\": {\n                    \"minimum\": 1,\n                    \"maximum\": max_workers,\n                },\n            }\n            tr = DaskTaskRunner(**dask_task_runner_kwargs)\n        else:\n            raise ValueError(\"Prefect task runner not recognized\")\n\n        @flow(task_runner=tr, name=self.name)\n        def prefect_main_wrapper():\n            self._check_env_and_run()\n\n        prefect_main_wrapper()\n\n    else:\n        logger = logging.getLogger(\"dataset\")\n        logger.setLevel(params.logger_level)\n        logger.addHandler(logging.StreamHandler())\n\n        if params.backend == \"mpi\":\n            from mpi4py import MPI\n\n            comm = MPI.COMM_WORLD\n            rank = comm.Get_rank()\n            if rank != 0:\n                return\n\n            self.backend = \"mpi\"\n            self.mpi_max_workers = max_workers\n\n            self._check_env_and_run()\n\n        elif params.backend == \"local\":\n            if params.run_parallel:\n                self.backend = \"concurrent\"\n            else:\n                self.backend = \"serial\"\n            self._check_env_and_run()\n\n        else:\n            raise ValueError(f\"Backend {params.backend} not recognized.\")\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.run_concurrent_tasks","title":"<code>run_concurrent_tasks(name, func, input_list, force_sequential, max_workers=None)</code>","text":"<p>Run tasks concurrently (locally), given a function a list of inputs This will always return a list of TaskResults!</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def run_concurrent_tasks(\n    self,\n    name: str,\n    func: Callable,\n    input_list: Iterable[Any],\n    force_sequential: bool,\n    max_workers: int = None,\n):\n    \"\"\"\n    Run tasks concurrently (locally), given a function a list of inputs\n    This will always return a list of TaskResults!\n    \"\"\"\n    if max_workers is None:\n        max_workers = self.max_workers\n    pool_size = 1 if force_sequential else max_workers\n    with multiprocessing.Pool(pool_size) as pool:\n        results = pool.starmap(\n            self.error_wrapper,\n            [(func, i) for i in input_list],\n            chunksize=self.chunksize,\n        )\n    return results\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.run_mpi_tasks","title":"<code>run_mpi_tasks(name, func, input_list, force_sequential, max_workers=None)</code>","text":"<p>Run tasks using MPI, requiring the use of <code>mpirun</code> self.pool is an MPIPoolExecutor initialized by self.run() This will always return a list of TaskResults!</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def run_mpi_tasks(\n    self,\n    name: str,\n    func: Callable,\n    input_list: Iterable[Any],\n    force_sequential: bool,\n    max_workers: int = None,\n):\n    \"\"\"\n    Run tasks using MPI, requiring the use of `mpirun`\n    self.pool is an MPIPoolExecutor initialized by self.run()\n    This will always return a list of TaskResults!\n    \"\"\"\n    from mpi4py.futures import MPIPoolExecutor\n\n    if not max_workers:\n        max_workers = self.mpi_max_workers\n\n    with MPIPoolExecutor(\n        max_workers=max_workers, chunksize=self.chunksize\n    ) as pool:\n        futures = []\n        for i in input_list:\n            f = pool.submit(self.error_wrapper, func, i)\n            if force_sequential:\n                wait([f])\n            futures.append(f)\n    return [f.result() for f in futures]\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.run_prefect_tasks","title":"<code>run_prefect_tasks(name, func, input_list, force_sequential)</code>","text":"<p>Run tasks using Prefect, using whichever task runner decided in self.run() This will always return a list of TaskResults!</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def run_prefect_tasks(\n    self,\n    name: str,\n    func: Callable,\n    input_list: Iterable[Any],\n    force_sequential: bool,\n):\n    \"\"\"\n    Run tasks using Prefect, using whichever task runner decided in self.run()\n    This will always return a list of TaskResults!\n    \"\"\"\n\n    from prefect import task\n\n    logger = self.get_logger()\n\n    task_wrapper = task(\n        func,\n        name=name,\n        retries=self.retries,\n        retry_delay_seconds=self.retry_delay,\n        persist_result=True,\n    )\n\n    futures = []\n    for i in input_list:\n        w = [i[1] for i in futures] if force_sequential else None\n        futures.append((i, task_wrapper.submit(*i, wait_for=w, return_state=False)))\n\n    results = []\n\n    states = [(i[0], i[1].wait()) for i in futures]\n\n    while states:\n        for ix, (inputs, state) in enumerate(states):\n            if state.is_completed():\n                # print('complete', ix, inputs)\n                logger.info(f\"complete - {ix} - {inputs}\")\n\n                results.append(TaskResult(0, \"Success\", inputs, state.result()))\n            elif state.is_failed() or state.is_crashed() or state.is_cancelled():\n                # print('fail', ix, inputs)\n                logger.info(f\"fail - {ix} - {inputs}\")\n\n                try:\n                    msg = repr(state.result(raise_on_failure=True))\n                except Exception as e:\n                    msg = f\"Unable to retrieve error message - {e}\"\n                results.append(TaskResult(1, msg, inputs, None))\n            else:\n                # print('not ready', ix, inputs)\n                continue\n            _ = states.pop(ix)\n        time.sleep(5)\n\n    # for inputs, future in futures:\n    #     state = future.wait(60*60*2)\n    #     if state.is_completed():\n    #         results.append(TaskResult(0, \"Success\", inputs, state.result()))\n    #     elif state.is_failed() or state.is_crashed():\n    #         try:\n    #             msg = repr(state.result(raise_on_failure=False))\n    #         except:\n    #             msg = \"Unable to retrieve error message\"\n    #         results.append(TaskResult(1, msg, inputs, None))\n    #     else:\n    #         pass\n\n    # while futures:\n    #     for ix, (inputs, future) in enumerate(futures):\n    #         state = future.get_state()\n    #         # print(repr(state))\n    #         # print(repr(future))\n    #         if state.is_completed():\n    #             print('complete', ix, inputs)\n    #             results.append(TaskResult(0, \"Success\", inputs, future.result()))\n    #         elif state.is_failed() or state.is_crashed() or state.is_cancelled():\n    #             print('fail', ix, inputs)\n    #             try:\n    #                 msg = repr(future.result(raise_on_failure=True))\n    #             except Exception as e:\n    #                 msg = f\"Unable to retrieve error message - {e}\"\n    #             results.append(TaskResult(1, msg, inputs, None))\n    #         else:\n    #             # print('not ready', ix, inputs)\n    #             continue\n    #         _ = futures.pop(ix)\n    #         # future.release()\n    #     time.sleep(5)\n\n    return results\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.run_serial_tasks","title":"<code>run_serial_tasks(name, func, input_list)</code>","text":"<p>Run tasks in serial (locally), given a function and list of inputs This will always return a list of TaskResults!</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def run_serial_tasks(self, name, func: Callable, input_list: Iterable[Any]):\n    \"\"\"\n    Run tasks in serial (locally), given a function and list of inputs\n    This will always return a list of TaskResults!\n    \"\"\"\n    logger = self.get_logger()\n    logger.debug(f\"run_serial_tasks - input_list: {input_list}\")\n    return [self.error_wrapper(func, i) for i in input_list]\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.Dataset.run_tasks","title":"<code>run_tasks(func, input_list, allow_futures=True, name=None, retries=3, retry_delay=60, force_sequential=False, force_serial=False, max_workers=None, prefect_concurrency_tag=None, prefect_concurrency_task_value=None)</code>","text":"<p>Run a bunch of tasks, calling one of the above run_tasks functions This is the function that should be called most often from self.main() It will return a ResultTuple of TaskResults</p> Source code in <code>data_manager/dataset.py</code> <pre><code>def run_tasks(\n    self,\n    func: Callable,\n    input_list: Iterable[Any],\n    allow_futures: bool = True,\n    name: Optional[str] = None,\n    retries: int = 3,\n    retry_delay: int = 60,\n    force_sequential: bool = False,\n    force_serial: bool = False,\n    max_workers: Optional[int] = None,\n    prefect_concurrency_tag: bool = None,\n    prefect_concurrency_task_value: bool = None,\n):\n    \"\"\"\n    Run a bunch of tasks, calling one of the above run_tasks functions\n    This is the function that should be called most often from self.main()\n    It will return a ResultTuple of TaskResults\n    \"\"\"\n\n    timestamp = datetime.today()\n\n    if not callable(func):\n        raise TypeError(\"Function passed to run_tasks is not callable\")\n\n    # Save global retry settings, and override with current values\n    old_retries, old_retry_delay = self.retries, self.retry_delay\n    self.retries, self.retry_delay = self.init_retries(retries, retry_delay)\n\n    logger = self.get_logger()\n\n    if name is None:\n        try:\n            name = func.__name__\n        except AttributeError:\n            logger.warning(\n                \"No name given for task run, and function does not have a name (multiple unnamed functions may result in log files being overwritten)\"\n            )\n            name = \"unnamed\"\n    elif not isinstance(name, str):\n        raise TypeError(\"Name of task run must be a string\")\n\n    if max_workers is None:\n        max_workers = self.max_workers\n\n    if self.backend == \"serial\" or force_serial:\n        results = self.run_serial_tasks(name, func, input_list)\n    elif self.backend == \"concurrent\":\n        results = self.run_concurrent_tasks(\n            name, func, input_list, force_sequential, max_workers=max_workers\n        )\n    elif self.backend == \"prefect\":\n        from prefect.concurrency.asyncio import concurrency\n        with concurrency(prefect_concurrency_tag, occupy=prefect_concurrency_task_value):\n            results = self.run_prefect_tasks(name, func, input_list, force_sequential)\n\n    elif self.backend == \"mpi\":\n        results = self.run_mpi_tasks(name, func, input_list, force_sequential, max_workers=max_workers)\n    else:\n        raise ValueError(\n            \"Requested backend not recognized. Have you called this Dataset's run function?\"\n        )\n\n    if len(results) == 0:\n        raise ValueError(\n            f\"Task run {name} yielded no results. Did it receive any inputs?\"\n        )\n\n    success_count = sum(1 for r in results if r.status_code == 0)\n    error_count = len(results) - success_count\n    if error_count == 0:\n        logger.info(\n            f\"Task run {name} completed with {success_count} successes and no errors\"\n        )\n    else:\n        logger.warning(\n            f\"Task run {name} completed with {error_count} errors and {success_count} successes\"\n        )\n\n    # Restore global retry settings\n    self.retries, self.retry_delay = old_retries, old_retry_delay\n\n    return ResultTuple(results, name, timestamp)\n</code></pre>"},{"location":"reference/data_manager/dataset/#data_manager.dataset.ResultTuple","title":"<code>ResultTuple</code>","text":"<p>               Bases: <code>Sequence</code></p> <p>This is an immutable sequence designed to hold TaskResults It also keeps track of the name of a run and the time it started ResultTuple.results() returns a list of results from each task</p> Source code in <code>data_manager/dataset.py</code> <pre><code>class ResultTuple(Sequence):\n    \"\"\"\n    This is an immutable sequence designed to hold TaskResults\n    It also keeps track of the name of a run and the time it started\n    ResultTuple.results() returns a list of results from each task\n    \"\"\"\n\n    def __init__(\n        self,\n        iterable: Iterable[TaskResult],\n        name: str,\n        timestamp: datetime = datetime.today(),\n    ):\n        self.elements = []\n        for value in iterable:\n            if isinstance(value, TaskResult):\n                self.elements.append(value)\n            else:\n                raise ValueError(\n                    \"ResultTuples must only consist of TaskResult namedtuples!\"\n                )\n        self.name = name\n        self.timestamp = timestamp\n\n    def __getitem__(self, key: int):\n        return self.elements[key]\n\n    def __len__(self):\n        return len(self.elements)\n\n    def __repr__(self):\n        success_count = sum(1 for t in self.elements if t.status_code == 0)\n        error_count = len(self.elements) - success_count\n        return f'&lt;ResultTuple named \"{self.name}\" with {success_count} successes, {error_count} errors&gt;'\n\n    def args(self):\n        args = [t.args for t in self.elements if t.status_code == 0]\n        if len(args) &lt; len(self.elements):\n            logging.getLogger(\"dataset\").warning(\n                f\"args() function for ResultTuple {self.name} skipping errored tasks\"\n            )\n        return args\n\n    def results(self):\n        results = [t.result for t in self.elements if t.status_code == 0]\n        if len(results) &lt; len(self.elements):\n            logging.getLogger(\"dataset\").warning(\n                f\"results() function for ResultTuple {self.name} skipping errored tasks\"\n            )\n        return results\n</code></pre>"}]}